{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Jupyter Notebook: main_analysis.ipynb\n",
    "# Ensure all dependencies are installed using requirements.txt prior to running this notebook.\n",
    "\n",
    "# Step 1: Setup and Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scripts.text_preprocessing import TextPreprocessor\n",
    "from scripts.data_exploration import plot_feature_distribution\n",
    "from scripts.sentiment_analysis import classify_sentiment, generate_wordcloud, ngram_analysis\n",
    "from scripts.sentiment_correlation import plot_correlation_heatmap\n",
    "from scripts.topic_modeling import train_lda_model, train_bertopic_model\n",
    "from scripts.sentiment_prediction import train_sentiment_model\n",
    "\n",
    "# Step 2: Load Data\n",
    "print(\"Loading data...\")\n",
    "DATA_PATH = 'data/senti_df.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Data loaded successfully!\")\n",
    "display(df.head())\n",
    "\n",
    "# Step 3: Text Preprocessing\n",
    "print(\"Starting text preprocessing...\")\n",
    "preprocessor = TextPreprocessor()\n",
    "df['cleaned_text'] = df['text'].apply(preprocessor.clean_text)\n",
    "print(\"Text preprocessing completed!\")\n",
    "display(df[['text', 'cleaned_text']].head())\n",
    "\n",
    "# Step 4: Initial Data Exploration\n",
    "print(\"Exploring data distributions...\")\n",
    "for feature in ['Speech_date', 'year', 'time', 'gender', 'party_group']:\n",
    "    if feature in df.columns:\n",
    "        plot_feature_distribution(df, feature)\n",
    "print(\"Data exploration completed!\")\n",
    "\n",
    "# Step 5: Sentiment Analysis and Classification\n",
    "print(\"Classifying sentiment and analyzing word frequencies...\")\n",
    "df = classify_sentiment(df, 'sentiment_score')  # Replace with your actual score column\n",
    "generate_wordcloud(df, 'positive')\n",
    "generate_wordcloud(df, 'negative')\n",
    "\n",
    "print(\"Running n-gram analysis for positive and negative speeches...\")\n",
    "ngram_analysis(df, 'positive', 2)  # Bi-gram analysis for positive speeches\n",
    "ngram_analysis(df, 'negative', 3)  # Tri-gram analysis for negative speeches\n",
    "\n",
    "print(\"Sentiment analysis completed!\")\n",
    "\n",
    "# Step 6: Correlation Analysis\n",
    "print(\"Calculating and plotting correlation heatmap...\")\n",
    "sentiment_columns = ['afinn_sentiment', 'jockers_sentiment', 'nrc_sentiment', 'huliu_sentiment', 'rheault_sentiment']\n",
    "if all(col in df.columns for col in sentiment_columns):\n",
    "    plot_correlation_heatmap(df, sentiment_columns)\n",
    "else:\n",
    "    print(\"Some sentiment columns are missing; skipping correlation heatmap.\")\n",
    "print(\"Correlation analysis completed!\")\n",
    "\n",
    "# Step 7: Topic Modeling (LDA and BERTopic)\n",
    "print(\"Training LDA model...\")\n",
    "lda_model, dictionary, corpus = train_lda_model(df, 'cleaned_text')\n",
    "print(\"LDA model training completed!\")\n",
    "\n",
    "print(\"Training BERTopic model...\")\n",
    "bertopic_model = train_bertopic_model(df, 'cleaned_text')\n",
    "print(\"BERTopic model training completed!\")\n",
    "\n",
    "# Step 8: Sentiment Prediction Model\n",
    "print(\"Training sentiment prediction model...\")\n",
    "train_sentiment_model(df, 'cleaned_text', 'sentiment')\n",
    "print(\"Sentiment prediction model training completed!\")\n",
    "\n",
    "# Step 9: Conclusion\n",
    "print(\"All tasks have been completed successfully!\")\n",
    "print(\"Make sure to review the visualizations and outputs for any further analysis.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
